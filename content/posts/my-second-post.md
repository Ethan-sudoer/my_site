+++
title = 'My Second Post'
date = 2024-05-08T19:45:24+08:00
draft = true
+++

2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（*SAC*）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。
	Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。
目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。

## 熵 Entropy
**熵**（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$

强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$


## 最大熵强化学习
#正则化
**最大熵强化学习**（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)+\alpha H(\pi(\cdot | s_t))]$$
其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。
	熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。


