<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>My First Post | My New Hugo Site</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="#policy-based
Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。
参数化建模策略 假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。
目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}{s_0}[V^{\pi\theta}(s_0)]$$ 其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。
策略梯度 ![[Pasted image 20230811153231.png]]
REINFORCE算法流程 初始化策略参数 $\theta$ 对于每一轮训练： 采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 蒙特卡洛方法 计算回报 更新参数 总结 可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。
评价REINFORCE REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。">
    <meta name="generator" content="Hugo 0.125.6">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/my-first-post/">
    

    <meta property="og:url" content="http://localhost:1313/posts/my-first-post/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="My First Post">
  <meta property="og:description" content="#policy-based
Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。
参数化建模策略 假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。
目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}{s_0}[V^{\pi\theta}(s_0)]$$ 其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。
策略梯度 ![[Pasted image 20230811153231.png]]
REINFORCE算法流程 初始化策略参数 $\theta$ 对于每一轮训练： 采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 蒙特卡洛方法 计算回报 更新参数 总结 可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。
评价REINFORCE REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-08T19:25:56+08:00">
    <meta property="article:modified_time" content="2024-05-08T19:25:56+08:00">

  <meta itemprop="name" content="My First Post">
  <meta itemprop="description" content="#policy-based
Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。
参数化建模策略 假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。
目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}{s_0}[V^{\pi\theta}(s_0)]$$ 其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。
策略梯度 ![[Pasted image 20230811153231.png]]
REINFORCE算法流程 初始化策略参数 $\theta$ 对于每一轮训练： 采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 蒙特卡洛方法 计算回报 更新参数 总结 可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。
评价REINFORCE REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。">
  <meta itemprop="datePublished" content="2024-05-08T19:25:56+08:00">
  <meta itemprop="dateModified" content="2024-05-08T19:25:56+08:00">
  <meta itemprop="wordCount" content="41"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="My First Post">
<meta name="twitter:description" content="#policy-based
Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。
参数化建模策略 假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。
目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}{s_0}[V^{\pi\theta}(s_0)]$$ 其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。
策略梯度 ![[Pasted image 20230811153231.png]]
REINFORCE算法流程 初始化策略参数 $\theta$ 对于每一轮训练： 采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 蒙特卡洛方法 计算回报 更新参数 总结 可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。
评价REINFORCE REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        My New Hugo Site
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">My First Post</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-05-08T19:25:56+08:00">May 8, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>#policy-based</p>
<p>Q-learning、DQN 及 DQN 改进算法都是<strong>基于价值</strong>（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是<strong>基于策略</strong>（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。</p>
<h2 id="参数化建模策略">参数化建模策略</h2>
<p>假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。</p>
<p>目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}<em>{s_0}[V^{\pi</em>\theta}(s_0)]$$
其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。</p>
<h2 id="策略梯度">策略梯度</h2>
<p>![[Pasted image 20230811153231.png]]</p>
<h2 id="reinforce算法流程">REINFORCE算法流程</h2>
<ol>
<li>初始化策略参数 $\theta$</li>
<li>对于每一轮训练：
<ol>
<li>采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 <strong>蒙特卡洛方法</strong></li>
<li>计算回报</li>
<li>更新参数</li>
</ol>
</li>
</ol>
<h2 id="总结">总结</h2>
<p>可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。</p>
<h3 id="评价reinforce">评价REINFORCE</h3>
<p>REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  My New Hugo Site 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
