<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 19:45:24 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My Second Post</title>
      <link>http://localhost:1313/posts/my-second-post/</link>
      <pubDate>Wed, 08 May 2024 19:45:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-second-post/</guid>
      <description>2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。 Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。 目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。&#xA;熵 Entropy 熵（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$&#xA;强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$&#xA;最大熵强化学习 #正则化 最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)+\alpha H(\pi(\cdot | s_t))]$$ 其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。 熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。</description>
    </item>
    <item>
      <title>My First Post</title>
      <link>http://localhost:1313/posts/my-first-post/</link>
      <pubDate>Wed, 08 May 2024 19:25:56 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-first-post/</guid>
      <description>#policy-based&#xA;Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。&#xA;参数化建模策略 假设目标策略 $\pi_\theta$是一个随机性策略，并且处处可微，$\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。&#xA;目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:$$J(\theta)=\mathbb{E}{s_0}[V^{\pi\theta}(s_0)]$$ 其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。&#xA;策略梯度 ![[Pasted image 20230811153231.png]]&#xA;REINFORCE算法流程 初始化策略参数 $\theta$ 对于每一轮训练： 采样当前基于当前策略的轨迹（尽可能达到目标长度）——采用 蒙特卡洛方法 计算回报 更新参数 总结 可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。&#xA;评价REINFORCE REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。</description>
    </item>
  </channel>
</rss>
