<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>My Second Post | My New Hugo Site</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。 Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。 目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。
熵 Entropy 熵（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$
强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$
最大熵强化学习 #正则化 最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)&#43;\alpha H(\pi(\cdot | s_t))]$$ 其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。 熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。">
    <meta name="generator" content="Hugo 0.125.6">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/my-second-post/">
    

    <meta property="og:url" content="http://localhost:1313/posts/my-second-post/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="My Second Post">
  <meta property="og:description" content="2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。 Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。 目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。
熵 Entropy 熵（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$
强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$
最大熵强化学习 #正则化 最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)&#43;\alpha H(\pi(\cdot | s_t))]$$ 其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。 熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-08T19:45:24+08:00">
    <meta property="article:modified_time" content="2024-05-08T19:45:24+08:00">

  <meta itemprop="name" content="My Second Post">
  <meta itemprop="description" content="2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。 Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。 目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。
熵 Entropy 熵（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$
强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$
最大熵强化学习 #正则化 最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)&#43;\alpha H(\pi(\cdot | s_t))]$$ 其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。 熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。">
  <meta itemprop="datePublished" content="2024-05-08T19:45:24+08:00">
  <meta itemprop="dateModified" content="2024-05-08T19:45:24+08:00">
  <meta itemprop="wordCount" content="35"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="My Second Post">
<meta name="twitter:description" content="2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。 Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。 目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。
熵 Entropy 熵（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$
强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$
最大熵强化学习 #正则化 最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)&#43;\alpha H(\pi(\cdot | s_t))]$$ 其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。 熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        My New Hugo Site
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">My Second Post</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-05-08T19:45:24+08:00">May 8, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（<em>SAC</em>）被提出。其前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。
Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。
目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。</p>
<h2 id="熵-entropy">熵 Entropy</h2>
<p><strong>熵</strong>（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果$X$ 是一个随机变量，且它的概率密度函数为$p$，那么它的熵就被定义为$$H(X)=E_{x\sim p}[-\log p(x)]$$</p>
<p>强化学习中可以类似定义一个策略的在某个状态下的随机程度（在一个状态下策略有多种可能的动作选择，他们构成了一个分布）$$H(\pi(\cdot | s_t))$$</p>
<h2 id="最大熵强化学习">最大熵强化学习</h2>
<p>#正则化
<strong>最大熵强化学习</strong>（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为$$\pi*=arg\max_{\pi}E_{\pi}[\sum_t r(s_t,a_t)+\alpha H(\pi(\cdot | s_t))]$$
其中，$\alpha$是一个正则化的系数，用来控制熵的重要程度。
熵正则化增加了强化学习算法的探索程度，$\alpha$越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  My New Hugo Site 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
